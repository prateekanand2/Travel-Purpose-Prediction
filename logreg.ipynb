{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns: (923572, 115)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV data\n",
    "file_path = 'csv/trippub.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "#REFRENCE CODES: https://nhts.ornl.gov/assets/2017/doc/codebook_v1.2.pdf\n",
    "\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of rows and columns:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 columns: Index(['HOUSEID', 'PERSONID', 'TDTRPNUM', 'STRTTIME', 'ENDTIME', 'TRVLCMIN',\n",
      "       'TRPMILES', 'TRPTRANS', 'TRPACCMP', 'TRPHHACC', 'VEHID', 'TRWAITTM',\n",
      "       'NUMTRANS', 'TRACCTM', 'DROP_PRK', 'TREGRTM', 'WHODROVE', 'WHYFROM',\n",
      "       'LOOP_TRIP', 'TRPHHVEH', 'HHMEMDRV', 'HH_ONTD', 'NONHHCNT', 'NUMONTRP',\n",
      "       'PSGR_FLG', 'PUBTRANS', 'TRIPPURP', 'DWELTIME', 'TDWKND', 'VMT_MILE',\n",
      "       'DRVR_FLG', 'WHYTRP1S', 'ONTD_P1', 'ONTD_P2', 'ONTD_P3', 'ONTD_P4',\n",
      "       'ONTD_P5', 'ONTD_P6', 'ONTD_P7', 'ONTD_P8', 'ONTD_P9', 'ONTD_P10',\n",
      "       'ONTD_P11', 'ONTD_P12', 'ONTD_P13', 'TDCASEID', 'TRACC_WLK',\n",
      "       'TRACC_POV', 'TRACC_BUS', 'TRACC_CRL'],\n",
      "      dtype='object')\n",
      "\n",
      "Next 50 columns: Index(['TRACC_SUB', 'TRACC_OTH', 'TREGR_WLK', 'TREGR_POV', 'TREGR_BUS',\n",
      "       'TREGR_CRL', 'TREGR_SUB', 'TREGR_OTH', 'WHYTO', 'TRAVDAY', 'HOMEOWN',\n",
      "       'HHSIZE', 'HHVEHCNT', 'HHFAMINC', 'DRVRCNT', 'HHSTATE', 'HHSTFIPS',\n",
      "       'NUMADLT', 'WRKCOUNT', 'TDAYDATE', 'HHRESP', 'LIF_CYC', 'MSACAT',\n",
      "       'MSASIZE', 'RAIL', 'URBAN', 'URBANSIZE', 'URBRUR', 'GASPRICE',\n",
      "       'CENSUS_D', 'CENSUS_R', 'CDIVMSAR', 'HH_RACE', 'HH_HISP', 'HH_CBSA',\n",
      "       'SMPLSRCE', 'R_AGE', 'EDUC', 'R_SEX', 'PRMACT', 'PROXY', 'WORKER',\n",
      "       'DRIVER', 'WTTRDFIN', 'WHYTRP90', 'TRPMILAD', 'R_AGE_IMP', 'R_SEX_IMP',\n",
      "       'VEHTYPE', 'OBHUR'],\n",
      "      dtype='object')\n",
      "\n",
      "Remaining columns: Index(['DBHUR', 'OTHTNRNT', 'OTPPOPDN', 'OTRESDN', 'OTEEMPDN', 'OBHTNRNT',\n",
      "       'OBPPOPDN', 'OBRESDN', 'DTHTNRNT', 'DTPPOPDN', 'DTRESDN', 'DTEEMPDN',\n",
      "       'DBHTNRNT', 'DBPPOPDN', 'DBRESDN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the first 50 columns\n",
    "print(\"First 50 columns:\", data.columns[:50])\n",
    "\n",
    "# Display the next 50 columns\n",
    "print(\"\\nNext 50 columns:\", data.columns[50:100])\n",
    "\n",
    "# Display the remaining columns\n",
    "print(\"\\nRemaining columns:\", data.columns[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "columns_to_drop = ['WHYTO', 'WHYFROM', 'TRIPPURP', 'WHYTRP1S', 'WHYTRP90', 'HH_CBSA']  # over half the entries are XXXXX, need to drop this\n",
    "data_dropped = data.drop(columns=columns_to_drop)\n",
    "data_dropped = data_dropped.map(lambda x: np.nan if ((isinstance(x, (int, float)) and x < 0) or (isinstance(x, str) and x == '-9')) else x)\n",
    "\n",
    "# Select columns with 'category' dtype\n",
    "categorical_columns = data_dropped.select_dtypes(include='object').columns\n",
    "print(len(categorical_columns))\n",
    "numerical_columns = data_dropped.select_dtypes(include='number').columns\n",
    "print(len(numerical_columns))\n",
    "\n",
    "for c in categorical_columns:\n",
    "    data_dropped[c] = data_dropped[c].fillna(data_dropped[c].mode())\n",
    "\n",
    "for n in numerical_columns:\n",
    "    data_dropped[n] = data_dropped[n].fillna(data_dropped[n].mean())\n",
    "\n",
    "data_dropped = pd.get_dummies(data_dropped)\n",
    "data_dropped = data_dropped.replace({True:1, False:0})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HOUSEID', 'PERSONID', 'TDTRPNUM', 'STRTTIME', 'ENDTIME', 'TRVLCMIN',\n",
      "       'TRPMILES', 'TRPTRANS', 'TRPACCMP', 'TRPHHACC',\n",
      "       ...\n",
      "       'OBHUR_C', 'OBHUR_R', 'OBHUR_S', 'OBHUR_T', 'OBHUR_U', 'DBHUR_C',\n",
      "       'DBHUR_R', 'DBHUR_S', 'DBHUR_T', 'DBHUR_U'],\n",
      "      dtype='object', length=167)\n",
      "0         20\n",
      "1          1\n",
      "2          1\n",
      "3         10\n",
      "4         20\n",
      "          ..\n",
      "923567    10\n",
      "923568    50\n",
      "923569    10\n",
      "923570    40\n",
      "923571     1\n",
      "Name: WHYTRP1S, Length: 923572, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Yes, let's use WHYTRP1S since WHYTO/WHYFROM split into too many categories I think it'll make classification harder.\n",
    "# TRIPPURP categories are not that useful and WHYTRP90 is from an older study, not necessary any more\n",
    "target_var = 'WHYTRP1S'\n",
    "\n",
    "X = data_dropped\n",
    "y = data[target_var]\n",
    "\n",
    "print(X.columns)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.choice(923572, size=20000, replace=False)\n",
    "X_sel = X.loc[rand,:]\n",
    "y_sel = y[rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.08057967, -0.70531177, -0.10613368, ..., -0.59900838,\n",
       "        -0.5482714 , -0.37893576],\n",
       "       [ 1.03562384, -0.70531177, -0.10613368, ..., -0.59900838,\n",
       "        -0.5482714 , -0.37893576],\n",
       "       [ 1.03080503,  0.37119193, -0.96682246, ..., -0.59900838,\n",
       "        -0.5482714 ,  2.63896974],\n",
       "       ...,\n",
       "       [ 1.10000055, -0.70531177,  2.90627704, ..., -0.59900838,\n",
       "        -0.5482714 , -0.37893576],\n",
       "       [-0.88092061,  0.37119193,  1.61524387, ...,  1.66942572,\n",
       "        -0.5482714 , -0.37893576],\n",
       "       [-0.91794613,  0.37119193, -0.96682246, ..., -0.59900838,\n",
       "        -0.5482714 , -0.37893576]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5743550875673334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "linreg = LogisticRegression(solver='newton-cholesky')\n",
    "linreg.fit(X_train_scale, y_train)\n",
    "y_pred = linreg.predict(X_test_scale)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "svc = LinearSVC(penalty='l1', dual=False, verbose=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5590666702758303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = svc.predict(X_test_scale)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LinearSVC_l1_C1.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(svc, 'LinearSVC_l1_C1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsvc = load('LinearSVC_l1_C1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5590666702758303\n"
     ]
    }
   ],
   "source": [
    "y_pred = newsvc.predict(X_test_scale)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009,\n",
       "       0.01 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.001, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
